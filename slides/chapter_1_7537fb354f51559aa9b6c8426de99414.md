---
title: Insert title here
key: 7537fb354f51559aa9b6c8426de99414

---
## Title Slide

```yaml
type: "TitleSlide"
key: "ef22a8a7e4"
```

`@lower_third`

name: Aditya Chaudhary
title: 


`@script`
Welcome back, in this section we'll discuss the applications of inverse and transpose in Machine Learning


---
## Why learn about Inverse and Transpose?

```yaml
type: "FullSlide"
key: "7761579732"
center_content: false
```

`@part1`
- Solve loss/cost functions
  
  - Ordinary Least Squares (OLS)

- Perform Principal Component Analysis (PCA) using Singular Value Decomposition (SVD)


`@script`
After all this background into inverse and transpose you might be thinking where can I apply these? They are very important concepts in the field of ML and are applied in nearly all the machine learning techniques. Primarily we'll talk about Ordinary Least Squares here and PCA in the next lesson.


---
## Ordinary Least Squares(OLS)

```yaml
type: "FullSlide"
key: "6b86ad26d4"
center_content: false
```

`@part1`
- Residual for Linear regression
![](https://assets.datacamp.com/production/repositories/4547/datasets/3605d1092576253f8677f0d637126b52b3beaf96/eqn1.JPG)

y: n x 1 vector of observations on dependent variable 
 
X: n x k matrix where we have observations on k 
independent variables for n observations

e: n x 1 vector of errors
  
β: k x 1 vector of unknown population parameters that we are trying to estimate


`@script`
In this section, we'll look at how inverse and transpose is used in minimizing the cost function for Ordinary Least Squares. 
The residual or error for OLS is given by the equation shown and all terms are either vectors or matrix. 
y is the dependent variable
X being the observations of independent variable
β is parameter we are trying to estimate
and e is the error or residual
Let us see a toy example to understand what we're doing here.


---
## Regression Example

```yaml
type: "TwoRows"
key: "10d29953f6"
```

`@part1`
- Importing libraries and getting the data {{1}}

```
import matplotlib.pyplot as plt
import numpy as np

bill = np.array([34,108,64,88,99,51])
tip =  np.array([5,17,11,8,14,5])
```{{1}}


`@part2`
- Getting the betas and prediction {{2}}


```
bill = bill.reshape(len(bill),1)
inv_bill = np.linalg.inv(bill.T.dot(bill))
beta = inv_bill.dot(bill.T).dot(tip)
yhat = bill.dot(beta) 
```{{2}}


`@script`
In this example, we try to model the relationship between restaurant bill and tips given on that bill. 
First, we import numpy and matplotlib and randomly get few numbers for bill and tips and save it in a numpy array which in this case is a one-dimensional vector.

We then calculate the intercepts with which we can fit a line to our vectors using a Normal equation, don't worry about it now. We'll cover it later.


---
## Regression Example

```yaml
type: "TwoColumns"
key: "b33da7620a"
```

`@part1`
- Plotting the data and the regression line {{1}}

```
plt.scatter(bill,tip)

plt.plot(bill,yhat,color='red')

plt.xlabel('Bill')

plt.ylabel('Tip')
```{{2}}


`@part2`
![](https://assets.datacamp.com/production/repositories/4547/datasets/3f335004be25ef32b035523176f91f92e0dbe9a9/LR.JPG){{2}}


`@script`
We plot the resulting line with our vectors and voila that's how you a fit a line to your data.

Now, we want to fit our line in such a way so as to minimize the error terms shown in the figure the sum of the square of which is our cost function. Let's see the math behind that.


---
## Residual Sum of Squares(RSS) for Ordinary Least Squares(OLS)

```yaml
type: "FullSlide"
key: "8650f87e3e"
disable_transition: false
```

`@part1`
- Residual Sum of Squares:{{1}}

![](https://assets.datacamp.com/production/repositories/4547/datasets/01b229234cb5531094b9ce023a3a3828fbd178b8/eqn4.JPG){{1}} 
 
- That makes our cost function:{{2}}

![](https://assets.datacamp.com/production/repositories/4547/datasets/86e06b6da390fede8dc968403c329e905946698a/eqn6.JPG){{2}}


`@script`
A vector multiplication between the residual and its transpose gives the residual sum of squares and that is what the cost function for Linear Regression is as we discussed.
Using the value for residual from a previous slide we can write our cost function as shown. 
We will minimize the cost function using pure linear algebra.


---
## Minimizing Cost Function

```yaml
type: "FullSlide"
key: "0bf3d46eb8"
```

`@part1`
- Solving the cost function

![](https://assets.datacamp.com/production/repositories/4547/datasets/f40e373a258098fd37131ca2ddc081bdc51b0f43/new1.JPG){{1}}

![](https://assets.datacamp.com/production/repositories/4547/datasets/36010b2d2591dbae8b04bedc2ea3da0759bc5b28/new2.JPG){{2}}

![](https://assets.datacamp.com/production/repositories/4547/datasets/02d6b5f6764dac64301d8cd36102b3c6023b051b/new3.JPG){{3}}


`@script`
We massage the equation that we have to prepare it to minimize. 
Xβ and y are vectors and their order can be shifted in multiplication and we get the cost function as we see. 
Finally, time to minimize!


---
## Minimizing Cost Function

```yaml
type: "FullSlide"
key: "9ce5533581"
```

`@part1`
- Minimizing the cost function

![](https://assets.datacamp.com/production/repositories/4547/datasets/a10525a6be30584c760b2d5b4d01f415344549dc/new4.JPG){{1}}

![](https://assets.datacamp.com/production/repositories/4547/datasets/0cdc2bee00546d4935f03f21308d068e9eda27bd/new5.JPG){{2}}

![](https://assets.datacamp.com/production/repositories/4547/datasets/ae38631d67210747fecd041ce6407737b160dd18/new6.JPG){{2}}

![](https://assets.datacamp.com/production/repositories/4547/datasets/6e8c1f0bee81eadcb86ddc294bf28135d0aa531d/new7.JPG){{3}}


`@script`
To give you a refresher of high school calculus, to find where a function attains minimum value we differentiate the function with respect to the unknown, in this case β and set it to 0. 

Now to get the value of β where it minimizes the cost function we multiply both sides by the inverse of X transpose X. As we know a matrix multiplied by its inverse gives us the identity matrix.

That leads us to.


---
## Normal Equation

```yaml
type: "FullSlide"
key: "f72efc9e23"
```

`@part1`
![](https://assets.datacamp.com/production/repositories/4547/datasets/c6d9d4940d59668589e44ca116e4742dd9de60be/last.JPG)


`@script`
The Normal equation!

We can use this equation to fit a straight line to our dataset as we saw in our toy example.


---
## Thank you!

```yaml
type: "FinalSlide"
key: "dd8e917d4d"
```

`@script`
We saw the application of inverse and transpose in Machine Learning and how important it is if we want to understand the functioning of Machine Learning techniques. In our next section we'll discuss advanced Linear Algebra and their application in ML

