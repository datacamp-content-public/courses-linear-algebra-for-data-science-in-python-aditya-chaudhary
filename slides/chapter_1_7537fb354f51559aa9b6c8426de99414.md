---
title: Insert title here
key: 7537fb354f51559aa9b6c8426de99414

---
## Title Slide

```yaml
type: "TitleSlide"
key: "ef22a8a7e4"
```

`@lower_third`

name: Aditya Chaudhary
title: 


`@script`



---
## Why learn about Inverse and Transpose?

```yaml
type: "FullSlide"
key: "7761579732"
center_content: false
```

`@part1`
- Solve loss/cost functions
  
  - Ordinary Least Squares (OLS)

- Perform Principal Component Analysis (PCA) using Singular Value Decomposition (SVD)


`@script`
After all this background into inverse and transpose you might be thinking where can I apply these. They are very important concepts in the field of ML and have their applications in a lot of places. Primarily we'll talk about Ordinary Least Squares and PCA in the next lesson.


---
## Residual Sum of Squares(RSS) for Ordinary Least Squares(OLS)

```yaml
type: "FullSlide"
key: "6b86ad26d4"
center_content: false
```

`@part1`
- Residual for Linear regression
![](https://assets.datacamp.com/production/repositories/4547/datasets/3605d1092576253f8677f0d637126b52b3beaf96/eqn1.JPG)

y: n x 1 vector of observations on dependent variable 
 
X: n x k matrix where we have observations on k 
independent variables for n observations

e: n x 1 vector of errors
  
β: k x 1 vector of unknown population parameters that we are trying to estimate


`@script`
You might have heard about Gradient Descent as an optimization algorithm for finding the minimum of a function. We use that for finding the minimum of OLS but that is not the only way to solve for OLS. Residual of Linear Regression is computed using the equation shown. Let us find the cost function to minimize. 

> 

> 

> 

> 

> 

>


---
## Insert title here...

```yaml
type: "TwoRows"
key: "10d29953f6"
```

`@part1`
```
import matplotlib.pyplot as plt
import numpy as np

bill = np.array([34,108,64,88,99,51])
tip =  np.array([5,17,11,8,14,5])
```{{1}}


`@part2`
```
bill = bill.reshape(len(bill),1)
inv_bill = np.linalg.inv(bill.T.dot(bill))
beta = inv_bill.dot(bill.T).dot(tip)
yhat = bill.dot(beta) 
```{{2}}


`@script`



---
## Insert title here...

```yaml
type: "TwoColumns"
key: "b33da7620a"
```

`@part1`
```
plt.scatter(bill,tip)

plt.plot(bill,yhat,color='red')

plt.xlabel('Bill')

plt.ylabel('Tip')
```{{1}}


`@part2`
![](https://assets.datacamp.com/production/repositories/4547/datasets/3f335004be25ef32b035523176f91f92e0dbe9a9/LR.JPG){{2}}


`@script`



---
## Residual Sum of Squares(RSS) for Ordinary Least Squares(OLS)

```yaml
type: "FullSlide"
key: "8650f87e3e"
```

`@part1`
- Residual Sum of Squares:
![](https://assets.datacamp.com/production/repositories/4547/datasets/01b229234cb5531094b9ce023a3a3828fbd178b8/eqn4.JPG)
- That makes our cost function:
![](https://assets.datacamp.com/production/repositories/4547/datasets/86e06b6da390fede8dc968403c329e905946698a/eqn6.JPG)


`@script`
A vector multiplication between the residual and it's transpose gives the residual sum of squares and that is what the cost function for Linear Regression is. We will try to solve for the minimum betas using pure linear algebra.


---
## Normal Equation for OLS

```yaml
type: "FullSlide"
key: "158f5336d0"
```

`@part1`
- Solving the equation by multiplying vectors 
![](https://assets.datacamp.com/production/repositories/4547/datasets/94584f06d59a6669be7d459bb699f6f944b3f0d7/2.JPG)


`@script`
Here we just multiply vectors to try to solve for beta
Xβ and y are vectors and their order can be shifted in multiplication.


---
## Normal Equation for OLS 

```yaml
type: "FullSlide"
key: "60818497eb"
```

`@part1`
- Minimizing the cost function

![](https://assets.datacamp.com/production/repositories/4547/datasets/6f37fe04bc864d69615128e33bbcec4374d2f254/3.JPG)


`@script`



---
## Final Slide

```yaml
type: "FinalSlide"
key: "dd8e917d4d"
```

`@script`


