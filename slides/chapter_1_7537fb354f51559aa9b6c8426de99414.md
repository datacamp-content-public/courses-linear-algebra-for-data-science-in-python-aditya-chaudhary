---
title: Insert title here
key: 7537fb354f51559aa9b6c8426de99414

---
## Title Slide

```yaml
type: "TitleSlide"
key: "ef22a8a7e4"
```

`@lower_third`

name: Aditya Chaudhary
title: 


`@script`



---
## Why learn about Inverse and Transpose?

```yaml
type: "FullSlide"
key: "7761579732"
center_content: false
```

`@part1`
- Solve loss/cost functions
  
  - Ordinary Least Squares (OLS)

- Perform Principal Component Analysis (PCA) using Singular Value Decomposition (SVD)


`@script`
After all this background into inverse and transpose you might be thinking where can I apply these. They are very important concepts in the field of ML and have their applications in a lot of places. Primarily we'll talk about Ordinary Least Squares here and PCA in the next lesson.


---
## Residual Sum of Squares(RSS) for Ordinary Least Squares(OLS)

```yaml
type: "FullSlide"
key: "6b86ad26d4"
center_content: false
```

`@part1`
- Residual for Linear regression
![](https://assets.datacamp.com/production/repositories/4547/datasets/3605d1092576253f8677f0d637126b52b3beaf96/eqn1.JPG)

y: n x 1 vector of observations on dependent variable 
 
X: n x k matrix where we have observations on k 
independent variables for n observations

e: n x 1 vector of errors
  
Î²: k x 1 vector of unknown population parameters that we are trying to estimate


`@script`
You might have heard about Gradient Descent as an optimization algorithm for finding the minimum of a function. We use that for finding the minimum of OLS but that is not the only way to solve for OLS. Residual of Linear Regression is computed using the equation shown. Let us see a toy example to understand what we're doing here.


---
## Regression Example

```yaml
type: "TwoRows"
key: "10d29953f6"
```

`@part1`
- Importing libraries and getting the data {{1}}

```
import matplotlib.pyplot as plt
import numpy as np

bill = np.array([34,108,64,88,99,51])
tip =  np.array([5,17,11,8,14,5])
```{{2}}


`@part2`
- Getting the betas and prediction {{3}}


```
bill = bill.reshape(len(bill),1)
inv_bill = np.linalg.inv(bill.T.dot(bill))
beta = inv_bill.dot(bill.T).dot(tip)
yhat = bill.dot(beta) 
```{{4}}


`@script`



---
## Regression Example

```yaml
type: "TwoColumns"
key: "b33da7620a"
```

`@part1`
- Plotting the data and the regression line {{1}}

```
plt.scatter(bill,tip)

plt.plot(bill,yhat,color='red')

plt.xlabel('Bill')

plt.ylabel('Tip')
```{{2}}


`@part2`
![](https://assets.datacamp.com/production/repositories/4547/datasets/3f335004be25ef32b035523176f91f92e0dbe9a9/LR.JPG){{3}}


`@script`
We want to fit our line in such a way so as to minimize the error terms shown in figure. Let's see the math behind that.


---
## Residual Sum of Squares(RSS) for Ordinary Least Squares(OLS)

```yaml
type: "FullSlide"
key: "8650f87e3e"
disable_transition: false
```

`@part1`
- Residual Sum of Squares:{{1}}

![](https://assets.datacamp.com/production/repositories/4547/datasets/01b229234cb5531094b9ce023a3a3828fbd178b8/eqn4.JPG){{2}} 
 
- That makes our cost function:{{3}}

![](https://assets.datacamp.com/production/repositories/4547/datasets/86e06b6da390fede8dc968403c329e905946698a/eqn6.JPG){{4}}


`@script`
A vector multiplication between the residual and it's transpose gives the residual sum of squares and that is what the cost function for Linear Regression is. We will try to solve for the minimum betas using pure linear algebra.


---
## Minimizing Cost Function

```yaml
type: "FullSlide"
key: "0bf3d46eb8"
```

`@part1`
- Solving the cost function

![](https://assets.datacamp.com/production/repositories/4547/datasets/f40e373a258098fd37131ca2ddc081bdc51b0f43/new1.JPG){{1}}

![](https://assets.datacamp.com/production/repositories/4547/datasets/36010b2d2591dbae8b04bedc2ea3da0759bc5b28/new2.JPG){{2}}

![](https://assets.datacamp.com/production/repositories/4547/datasets/02d6b5f6764dac64301d8cd36102b3c6023b051b/new3.JPG){{3}}


`@script`



---
## Minimizing Cost Function

```yaml
type: "FullSlide"
key: "9ce5533581"
```

`@part1`
- Minimizing the cost function

![](https://assets.datacamp.com/production/repositories/4547/datasets/a10525a6be30584c760b2d5b4d01f415344549dc/new4.JPG){{1}}

![](https://assets.datacamp.com/production/repositories/4547/datasets/0cdc2bee00546d4935f03f21308d068e9eda27bd/new5.JPG){{2}}

![](https://assets.datacamp.com/production/repositories/4547/datasets/ae38631d67210747fecd041ce6407737b160dd18/new6.JPG){{3}}

![](https://assets.datacamp.com/production/repositories/4547/datasets/6e8c1f0bee81eadcb86ddc294bf28135d0aa531d/new7.JPG){{4}}


`@script`



---
## Normal Equation

```yaml
type: "FullSlide"
key: "f72efc9e23"
```

`@part1`
![](https://assets.datacamp.com/production/repositories/4547/datasets/c6d9d4940d59668589e44ca116e4742dd9de60be/last.JPG){{1}}

- Solves Linear Regression problems {{2}}


`@script`



---
## Thank you!

```yaml
type: "FinalSlide"
key: "dd8e917d4d"
```

`@script`


